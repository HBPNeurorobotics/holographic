@Article{Brette2007,
author="Brette, Romain
and Rudolph, Michelle
and Carnevale, Ted
and Hines, Michael
and Beeman, David
and Bower, James M.
and Diesmann, Markus
and Morrison, Abigail
and Goodman, Philip H.
and Harris, Frederick C.
and Zirpe, Milind
and Natschl{\"a}ger, Thomas
and Pecevski, Dejan
and Ermentrout, Bard
and Djurfeldt, Mikael
and Lansner, Anders
and Rochel, Olivier
and Vieville, Thierry
and Muller, Eilif
and Davison, Andrew P.
and El Boustani, Sami
and Destexhe, Alain",
title="Simulation of networks of spiking neurons: A review of tools and strategies",
journal="Journal of Computational Neuroscience",
year="2007",
volume="23",
number="3",
pages="349--398",
abstract="We review different aspects of the simulation of spiking neural networks. We start by reviewing the different types of simulation strategies and algorithms that are currently implemented. We next review the precision of those simulation strategies, in particular in cases where plasticity depends on the exact timing of the spikes. We overview different simulators and simulation environments presently available (restricted to those freely available, open source and documented). For each simulation tool, its advantages and pitfalls are reviewed, with an aim to allow the reader to identify which simulator is appropriate for a given task. Finally, we provide a series of benchmark simulations of different types of networks of spiking neurons, including Hodgkin--Huxley type, integrate-and-fire models, interacting with current-based or conductance-based synapses, using clock-driven or event-driven integration strategies. The same set of models are implemented on the different simulators, and the codes are made available. The ultimate goal of this review is to provide a resource to facilitate identifying the appropriate integration strategy and simulation tool to use for a given modeling problem related to spiking neural networks.",
issn="1573-6873",
doi="10.1007/s10827-007-0038-6",
url="http://dx.doi.org/10.1007/s10827-007-0038-6"
}

@article{Ponulak2011,
    abstract = {{
                The concept that neural information is encoded in the firing rate of neurons has been the dominant paradigm in neurobiology for many years. This paradigm has also been adopted by the theory of artificial neural networks. Recent physiological experiments demonstrate, however, that in many parts of the nervous system, neural code is founded on the timing of individual action potentials. This finding has given rise to the emergence of a new class of neural models, called spiking neural networks. In this paper we summarize basic properties of spiking neurons and spiking networks. Our focus is, specifically, on models of spike-based information coding, synaptic plasticity and learning. We also survey real-life applications of spiking models. The paper is meant to be an introduction to spiking neural networks for scientists from various disciplines interested in spike-based neural processing.
            }},
    author = {Ponulak, Filip and Kasinski, Andrzej},
    issn = {1689-0035},
    journal = {Acta neurobiologiae experimentalis},
    number = {4},
    pages = {409--433},
    pmid = {22237491},
    posted-at = {2014-02-06 10:48:36},
    priority = {2},
    title = {{Introduction to spiking neural networks: Information processing, learning and applications.}},
    url = {http://view.ncbi.nlm.nih.gov/pubmed/22237491},
    volume = {71},
    year = {2011}
}

@ARTICLE{Furber2013, 
author={Furber, S.B. and Lester, D.R. and Plana, L.A. and Garside, J.D. and Painkras, E. and Temple, S. and Brown, A.D.}, 
journal={Computers, IEEE Transactions on}, 
title={Overview of the SpiNNaker System Architecture}, 
year={2013}, 
volume={62}, 
number={12}, 
pages={2454-2467}, 
keywords={failure analysis;microprocessor chips;neural net architecture;parallel machines;synchronisation;ARM9 cores;SpiNNaker system architecture;billion neurons;bisection bandwidth;component failures;custom interconnect fabric;day-to-day operation;design philosophy;determinism;fault detection;flagship goal;memory coherence;million-core computing engine;parallel machine design;principal axioms;recovery mechanisms;spiking neural network architecture;synchronicity;Biological system modeling;Computer architecture;Network architecture;Neural networks;Program processors;Interconnection architectures;neurocomputers;parallel processors;real-time distributed}, 
doi={10.1109/TC.2012.142}, 
ISSN={0018-9340}, 
month={Dec},}

@ARTICLE{Hanuschkin2010,
  
 AUTHOR={Hanuschkin, Alexander  and  Kunkel, Susanne  and  Helias, Moritz  and  Morrison, Abigail  and  Diesmann, Markus},   
	 
TITLE={A general and efficient method for incorporating precise spike times in globally time-driven simulations},      
	
JOURNAL={Frontiers in Neuroinformatics},      
	
VOLUME={4},      
	
YEAR={2010},      
	
NUMBER={113},     
	  
URL={http://www.frontiersin.org/neuroinformatics/10.3389/fninf.2010.00113/abstract},       
	
DOI={10.3389/fninf.2010.00113},      
	
ISSN={1662-5196} ,      
	
ABSTRACT={Traditionally, event-driven simulations have been limited to the very restricted class of neuronal models for which the timing of future spikes can be expressed in closed form. Recently, the class of models that is amenable to event-driven simulation has been extended by the development of techniques to accurately calculate firing times for some integrate-and-fire neuron models that do not enable the prediction of future spikes in closed form. The motivation of this development is the general perception that time-driven simulations are imprecise. Here, we demonstrate that a globally time-driven scheme can calculate firing times that cannot be discriminated from those calculated by an event-driven implementation of the same model; moreover, the time-driven scheme incurs lower computational costs. The key insight is that time-driven methods are based on identifying a threshold crossing in the recent past, which can be implemented by a much simpler algorithm than the techniques for predicting future threshold crossings that are necessary for event-driven approaches. As run time is dominated by the cost of the operations performed at each incoming spike, which includes spike prediction in the case of event-driven simulation and retrospective detection in the case of time-driven simulation, the simple time-driven algorithm outperforms the event-driven approaches. Additionally, our method is generally applicable to all commonly used integrate-and-fire neuronal models; we show that a non-linear model employing a standard adaptive solver can reproduce a reference spike train with a high degree of precision.}}

@ARTICLE{Gewaltig:NEST,
  author = {Marc-Oliver Gewaltig and Markus Diesmann},
  title = {NEST (NEural Simulation Tool)},
  journal = {Scholarpedia},
  year = {2007},
  volume = {2},
  pages = {1430},
  number = {4}
}

@article{Markram2015,
    author = {Markram, Henry and Muller, Eilif and Ramaswamy, Srikanth and Reimann, Michael W. and Abdellah, Marwan and Sanchez, Carlos A. and Ailamaki, Anastasia and Alonso-Nanclares, Lidia and Antille, Nicolas and Arsever, Selim and Kahou, Guy A. and Berger, Thomas K. and Bilgili, Ahmet and Buncic, Nenad and Chalimourda, Athanassia and Chindemi, Giuseppe and Courcol, Jean-Denis and Delalondre, Fabien and Delattre, Vincent and Druckmann, Shaul and Dumusc, Raphael and Dynes, James and Eilemann, Stefan and Gal, Eyal and Gevaert, Michael E. and Ghobril, Jean-Pierre and Gidon, Albert and Graham, Joe W. and Gupta, Anirudh and Haenel, Valentin and Hay, Etay and Heinis, Thomas and Hernando, Juan B. and Hines, Michael and Kanari, Lida and Keller, Daniel and Kenyon, John and Khazen, Georges and Kim, Yihwa and King, James G. and Kisvarday, Zoltan and Kumbhar, Pramod and Lasserre, S\'{e}bastien and Le B\'{e}, Jean-Vincent and Magalh\~{a}es, Bruno R. C. and Merch\'{a}n-P\'{e}rez, Angel and Meystre, Julie and Morrice, Benjamin R. and Muller, Jeffrey and Mu\~{n}oz-C\'{e}spedes, Alberto and Muralidhar, Shruti and Muthurasa, Keerthan and Nachbaur, Daniel and Newton, Taylor H. and Nolte, Max and Ovcharenko, Aleksandr and Palacios, Juan and Pastor, Luis and Perin, Rodrigo and Ranjan, Rajnish and Riachi, Imad and Rodr\'{\i}guez, Jos\'{e}-Rodrigo and Riquelme, Juan L. and R\"{o}ssert, Christian and Sfyrakis, Konstantinos and Shi, Ying and Shillcock, Julian C. and Silberberg, Gilad and Silva, Ricardo and Tauheed, Farhan and Telefont, Martin and Toledo-Rodriguez, Maria and Tr\"{a}nkler, Thomas and Van Geit, Werner and D\'{\i}az, Jafet V. and Walker, Richard and Wang, Yun and Zaninetta, Stefano M. and DeFelipe, Javier and Hill, Sean L. and Segev, Idan and Sch\"{u}rmann, Felix},
    citeulike-article-id = {13798243},
    citeulike-linkout-0 = {http://www.cell.com/cell/abstract/S0092-8674(15)01191-5},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.cell.2015.09.029},
    day = {9},
    doi = {10.1016/j.cell.2015.09.029},
    issn = {00928674},
    journal = {Cell},
    keywords = {cortical-microcircuit},
    month = oct,
    number = {2},
    pages = {456--492},
    posted-at = {2015-10-09 16:01:18},
    priority = {4},
    publisher = {Elsevier},
    title = {{Reconstruction and Simulation of Neocortical Microcircuitry}},
    url = {http://dx.doi.org/10.1016/j.cell.2015.09.029},
    volume = {163},
    year = {2015}
}

@article{Baladron2015,
  author    = {Javier Baladron and
               Fred Henrik Hamker},
  title     = {A spiking neural network based on the basal ganglia functional anatomy},
  journal   = {Neural Networks},
  volume    = {67},
  pages     = {1--13},
  year      = {2015},
  url       = {http://dx.doi.org/10.1016/j.neunet.2015.03.002},
  doi       = {10.1016/j.neunet.2015.03.002},
  timestamp = {Mon, 15 Jun 2015 07:59:45 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/nn/BaladronH15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Billaudelle2015,
  author    = {Sebastian Billaudelle and
               Subutai Ahmad},
  title     = {Porting {HTM} Models to the Heidelberg Neuromorphic Computing Platform},
  journal   = {CoRR},
  volume    = {abs/1505.02142},
  year      = {2015},
  url       = {http://arxiv.org/abs/1505.02142},
  timestamp = {Mon, 01 Jun 2015 14:13:54 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BillaudelleA15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@online{SpinnWeb,
  title		= {Research Groups: APT - Advanced Processor Technologies (School of Computer Science - The University of Manchester)},
  url = {http://apt.cs.manchester.ac.uk/projects/SpiNNaker/},
  urldate		= {2016-02-11}
}

@online{BrianWeb,
  title		= {The Brian spiking neural network simulator},
  url = {http://briansimulator.org/},
  urldate		= {2016-02-11}
}

@online{SpikeyWeb,
  title		= {The "Spikey" Chip},
  url = {http://www.kip.uni-heidelberg.de/cms/vision/projects/facets/neuromorphic_hardware/single_chip_system/the_spikey_chip/},
  urldate		= {2016-02-11}
}

@online{GazeboWeb,
  title		= {Gazebo},
  url = {http://gazebosim.org/},
  urldate		= {2016-02-11}
}

@online{RosWeb,
  title		= {ROS},
  url = {http://www.ros.org/},
  urldate		= {2016-02-11}
}

@online{PynnWeb,
  title		= {PyNN},
  url = {http://neuralensemble.org/trac/PyNN},
  urldate		= {2016-02-11}
}

@online{HBP,
  title = {Human Brain Project},
  url= {https://www.humanbrainproject.eu/},
  urldate = {2016-06-03}
}

@online{NRP,
  title = {Neurorobotics Platform},
  url = {http://www.neurorobotics.net/},
  urldate = {2016-06-03}
}

@book{neuronbook,
author = "Nicholas T. Carnevale and Michael L. Hines",
title = {The NEURON Book}, 
publisher = {Cambridge University Press}, 
year = {2006},
isbn = {9780511541612}, 
note = {Cambridge Books Online}, 
url = {http://dx.doi.org/10.1017/CBO9780511541612}
}

@inproceedings{spinnMLP,
 author = {Jin, Xin and Luj\'{a}n, Mikel and Plana, Luis A. and Rast, Alexander D. and Welbourne, Stephen R. and Furber, Steve B.},
 title = {Efficient Parallel Implementation of Multilayer Backpropagation Networks on SpiNNaker},
 booktitle = {Proceedings of the 7th ACM International Conference on Computing Frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {89--90},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787297},
 doi = {10.1145/1787275.1787297},
 acmid = {1787297},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {backpropagation, mapping, mlp, parallel, pipeline, spinnaker},
}

@ARTICLE{Nengo,
  
 AUTHOR={Bekolay, Trevor  and  Bergstra, James  and  Hunsberger, Eric  and  DeWolf, Travis  and  Stewart, Terrence C  and  Rasmussen, Daniel  and  Choo, Xuan  and  Voelker, Aaron  and  Eliasmith, Chris},   
	 
TITLE={Nengo: A Python tool for building large-scale functional brain models},      
	
JOURNAL={Frontiers in Neuroinformatics},      
	
VOLUME={7},      
	
YEAR={2014},      
	
NUMBER={48},     
	  
URL={http://www.frontiersin.org/neuroinformatics/10.3389/fninf.2013.00048/abstract},       
	
DOI={10.3389/fninf.2013.00048},      
	
ISSN={1662-5196} ,      
	
ABSTRACT={Neuroscience currently lacks a comprehensive theory of how cognitive processes can be implemented in a biological substrate. The Neural Engineering Framework (NEF) proposes one such theory, but has not yet gathered significant empirical support, partly due to the technical challenge of building and simulating large-scale models with the NEF. Nengo is a software tool that can be used to build and simulate large-scale models based on the NEF; currently, it is the primary resource for both teaching how the NEF is used, and for doing research that generates specific NEF models to explain experimental data. Nengo 1.4, which was implemented in Java, was used to create Spaun, the world's largest functional brain model (Eliasmith et al., 2012). Simulating Spaun highlighted limitations in Nengo 1.4's ability to support model construction with simple syntax, to simulate large models quickly, and to collect large amounts of data for subsequent analysis. This paper describes Nengo 2.0, which is implemented in Python and overcomes these limitations. It uses simple and extendable syntax, simulates a benchmark model on the scale of Spaun 50 times faster than Nengo 1.4, and has a flexible mechanism for collecting simulation results.}}

@article{Vogels2005,
    address = {Volen Center for Complex Systems and Department of Biology, Brandeis University, Waltham, Massachusetts 02454-9110.},
    author = {Vogels, Tim P. P. and Abbott, L. F. F.},
    citeulike-article-id = {400135},
    citeulike-linkout-0 = {http://dx.doi.org/10.1523/JNEUROSCI.3508-05.2005},
    citeulike-linkout-1 = {http://dx.doi.org/10.1523/jneurosci.3508-05.2005},
    citeulike-linkout-2 = {http://www.jneurosci.org/content/25/46/10786.abstract},
    citeulike-linkout-3 = {http://www.jneurosci.org/content/25/46/10786.full.pdf},
    citeulike-linkout-4 = {http://view.ncbi.nlm.nih.gov/pubmed/16291952},
    citeulike-linkout-5 = {http://www.hubmed.org/display.cgi?uids=16291952},
    day = {16},
    doi = {10.1523/jneurosci.3508-05.2005},
    issn = {1529-2401},
    journal = {Journal of Neuroscience},
    keywords = {network, plasticity},
    month = nov,
    number = {46},
    pages = {10786--10795},
    pdf = {vogels\_05\_signal.pdf},
    pmid = {16291952},
    posted-at = {2005-12-08 21:20:51},
    priority = {2},
    publisher = {Society for Neuroscience},
    title = {{Signal Propagation and Logic Gating in Networks of Integrate-and-Fire Neurons}},
    url = {http://dx.doi.org/10.1523/JNEUROSCI.3508-05.2005},
    volume = {25},
    year = {2005}
}

@misc{Hawkins2015,
    archivePrefix = {arXiv},
    author = {Hawkins, Jeff and Ahmad, Subutai},
    citeulike-article-id = {13828786},
    citeulike-linkout-0 = {http://arxiv.org/abs/1511.00083},
    citeulike-linkout-1 = {http://arxiv.org/pdf/1511.00083},
    day = {31},
    eprint = {1511.00083},
    keywords = {neural\_network\_models},
    month = oct,
    posted-at = {2015-11-16 01:29:18},
    priority = {2},
    title = {{Why Neurons Have Thousands of Synapses, A Theory of Sequence Memory in Neocortex}},
    url = {http://arxiv.org/abs/1511.00083},
    year = {2015}
}

@ARTICLE{Davison2008,
  
 AUTHOR={Davison, Andrew P  and  Brüderle, Daniel  and  Eppler, Jochen M  and  Kremkow, Jens  and  Muller, Eilif  and  Pecevski, Dejan  and  Perrinet, Laurent  and  Yger, Pierre},   
	 
TITLE={PyNN: a common interface for neuronal network simulators},      
	
JOURNAL={Frontiers in Neuroinformatics},      
	
VOLUME={2},      
	
YEAR={2009},      
	
NUMBER={11},     
	  
URL={http://www.frontiersin.org/neuroinformatics/10.3389/neuro.11.011.2008/abstract},       
	
DOI={10.3389/neuro.11.011.2008},      
	
ISSN={1662-5196} ,      
	
ABSTRACT={Computational neuroscience has produced a diversity of software for simulations of networks of spiking neurons, with both negative and positive consequences. On the one hand, each simulator uses its own programming or configuration language, leading to considerable difficulty in porting models from one simulator to another. This impedes communication between investigators and makes it harder to reproduce and build on the work of others.  On the other hand, simulation results can be cross-checked between different simulators, giving greater confidence in their correctness, and each simulator has different optimizations, so the most appropriate simulator can be chosen for a given modelling task.   A common programming interface to multiple simulators would reduce or eliminate the problems of simulator diversity while retaining the benefits. PyNN is such an interface, making it possible to write a simulation script once, using the Python programming language, and run it without modification on any supported simulator (currently NEURON, NEST, PCSIM, Brian and the Heidelberg VLSI neuromorphic hardware). PyNN increases the productivity of neuronal network modelling by providing high-level abstraction, by promoting code sharing and reuse, and by  providing a foundation for simulator-agnostic analysis, visualization, and data-management tools. PyNN increases the reliability of modelling studies by making it much easier to check results on multiple simulators. PyNN is open-source software and is available from http://neuralensemble.org/PyNN.}}

@ARTICLE{Ekeberg2008,
AUTHOR={Ekeberg, {\"O}rjan and Djurfeldt, Mikael},
TITLE={MUSIC---Multisimulation Coordinator: Request For Comments},
YEAR={2008},
JOURNAL={Nature Precedings},
URL={http://dx.doi.org/10.1038/npre.2008.1830.1},
DOI={0.1038/npre.2008.1830.1}}

@ARTICLE{Furber2014,
author={S. B. Furber and F. Galluppi and S. Temple and L. A. Plana},
journal={Proceedings of the IEEE},
title={The SpiNNaker Project},
year={2014},
volume={102},
number={5},
pages={652-665},
keywords={multiprocessing systems;neural net architecture;parallel processing;real-time systems;SpiNNaker project;data packets;interconnect architecture;mammalian brain;parallel million-core computer;real-time event-driven programming model;spiking neural network architecture;Brain modeling;Computational modeling;Computer architecture;Multitasking;Neural networks;Neuroscience;Parallel programming;Program processors;Brain modeling;multicast algorithms;multiprocessor interconnection networks;neural network hardware;parallel programming},
doi={10.1109/JPROC.2014.2304638},
ISSN={0018-9219},
month={May},}
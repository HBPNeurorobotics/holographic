Automatically generated by Mendeley Desktop 1.16.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Eliasmith:2012:LargeScaleModel,
abstract = {A central challenge for cognitive and systems neuroscience is to relate the incredibly complex behavior of animals to the equally complex activity of their brains. Recently described, large-scale neural models have not bridged this gap between neural activity and biological function. In this work, we present a 2.5-million-neuron model of the brain (called "Spaun") that bridges this gap by exhibiting many different behaviors. The model is presented only with visual image sequences, and it draws all of its responses with a physically modeled arm. Although simplified, the model captures many aspects of neuroanatomy, neurophysiology, and psychological behavior, which we demonstrate via eight diverse tasks.},
author = {Eliasmith, Chris and Stewart, Terrence C and Choo, Xuan and Bekolay, Trevor and DeWolf, Travis and Tang, Charlie and Rasmussen, Daniel and Tang, Yichuan and Tang, Charlie and Rasmussen, Daniel},
doi = {10.1126/science.1225266},
file = {:home/ksiks/temp/eliasmith.2012.pdf:pdf},
isbn = {0036-8075},
issn = {0036-8075},
journal = {Science},
keywords = {Behavior,Brain,Brain: anatomy {\&} histology,Brain: physiology,Humans,Models,Neural Networks (Computer),Neurological,Software},
number = {6111},
pages = {1202--1205},
pmid = {1000106157},
title = {{A Large-Scale Model of the Functioning Brain}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1225266$\backslash$nhttp://www.ncbi.nlm.nih.gov/pubmed/23197532},
volume = {338},
year = {2012}
}
@article{Patyk-Lonska:2011:DistributedRepresentationsBased,
abstract = {Authors revise the concept of a distributed representation of data as well as two previously developed mod- els: Holographic Reduced Representation (HRR) and Binary Spatter Codes (BSC). A Geometric Analogue (GAc — "c" stands for continuous as opposed to its discrete version) of HRR is introduced – it employs role-filler binding based on geometric products. Atomic objects are real-valued vectors in n-dimensional Euclidean space while complex data structures belong to a hierarchy of multivectors. The paper reports on a test aimed at comparison of GAc with HRR and BSC. The test is analogous to the one proposed by Tony Plate in the mid 90s. We repeat Plate's test on GAc and compare the results with the original HRR and BSC — we concentrate on comparison of recognition percentage for the three models for comparable data size, rather than on the time taken to achieve high percentage. Results show that the best models for storing and recognizing multiple similar structures are GAc and BSC with recognition percentage highly above 90. The paper ends with remarks on perspective applications of geometric algebra to quantum algorithms.},
author = {Patyk-L{\'{o}}nska, Agnieszka and Czachor, Marek and Aerts, Diederik},
file = {:home/ksiks/temp/10.1.1.298.6032.pdf:pdf},
issn = {03505596},
journal = {Informatica (Ljubljana)},
keywords = {BSC,Distributed representation of data,Geometric algebra,HRR,Scaling},
number = {4},
pages = {407--417},
title = {{Distributed representations based on geometric Algebra: The continuous model}},
volume = {35},
year = {2011}
}
@article{Kleyko:2016:HolographicGraphNeuron,
abstract = {This article proposes the use of Vector Symbolic Architectures for implementing Hierarchical Graph Neuron, an architecture for memorizing patterns of generic sensor stimuli. The adoption of a Vector Symbolic representation ensures a one-layered design for the approach, while maintaining the previously reported properties and performance characteristics of Hierarchical Graph Neuron, and also improving the noise resistance of the architecture. The proposed architecture enables a linear (with respect to the number of stored entries) time search for an arbitrary sub-pattern.},
archivePrefix = {arXiv},
arxivId = {1501.03784},
author = {Kleyko, Denis and Osipov, Evgeny and Senior, Alexander and Khan, Asad I. and Sekercioglu, Yasar Ahmet},
doi = {10.1109/TNNLS.2016.2535338},
eprint = {1501.03784},
file = {:home/ksiks/temp/1501.03784v1.pdf:pdf},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
pages = {1--9},
title = {{Holographic Graph Neuron: A Bioinspired Architecture for Pattern Processing}},
year = {2016}
}
@inbook{Kanerva:1994:SpatterCodeEncoding,
abstract = {The Spatter Code is a high-dimensional (e.g., N = 10,000), random$\backslash$ncode that encodes “high-level concepts” in terms of their “low-level$\backslash$nattributes” so that concepts at different levels can be mixed freely. The binary$\backslash$nspatter code is the simplest. It has two N-bit codewords for each concept or$\backslash$nitem, a “high-level,” or dense, word with many randomly placed 1s and a$\backslash$n“low-level,” or sparse, word with a few (that are contained in the many). The$\backslash$ndense codewords can be used as inputs to an associative memory. The sparse$\backslash$ncodewords are used in encoding new concepts. When several items (attributes,$\backslash$nconcepts, chunks) are combined to form a new item, the two codewords for$\backslash$nthe new item are made from the sparse codewords of its constituents as follows:$\backslash$nthe new dense word is the logical {\{}OR{\}} of the constituents (i.e., their sum$\backslash$nthresholded at 0.5), and the new sparse word has 1s where the constituent$\backslash$nwords overlap (i.e., their sum thresholded at 1.5). When the parameters for the$\backslash$ncode are chosen properly, the number of 1s in the codewords is maintained as$\backslash$nnew items are encoded from combinations of old ones.},
address = {London},
author = {Kanerva, Pentti},
booktitle = {Icann'94},
doi = {10.1007/978-1-4471-2097-1_52},
editor = {Marinaro, Maria and Morasso, Pietro G},
file = {:home/ksiks/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanerva - 1994 - The spatter code for encoding concepts at many levels.pdf:pdf},
isbn = {978-3-540-19887-1},
pages = {226--229},
publisher = {Springer London},
title = {{The spatter code for encoding concepts at many levels}},
url = {http://link.springer.com/chapter/10.1007/978-1-4471-2097-1{\_}52},
year = {1994}
}
@article{Levy:2008:VectorSymbolicArchitectures,
abstract = {We provide an overviewofVector Symbolic Architectures (VSA), a class of structured associative memory models that offers a number of desirable features for artificial general intelligence. By directly encoding structure using familiar, computationally efficient algorithms, VSA bypasses many of the problems that have consumed unnecessary effort and attention in previous connectionist work. Example applications from opposite ends of the AI spectrum – visual map-seeking circuits and structured analogy processing – attest to the generality and power of the VSA approach in building new solutions for AI.},
author = {Levy, Simon D and Gayler, Ross W.},
file = {:home/ksiks/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy, Gayler - Unknown - Vector Symbolic Architectures A New Building Material for Artificial General Intelligence 1.pdf:pdf},
isbn = {978-1-58603-833-5},
issn = {09226389},
journal = {Proceedings of the First Conference on Artificial General Intelligence (AGI-08)},
keywords = {associative memory,binary spatter codes,connectionism,distributed representations,holographic reduced representation,vector symbolic architectures},
pages = {414--418},
pmid = {1000185213},
title = {{Vector Symbolic Architectures: A New Building Material for Artificial General Intelligence}},
url = {www.cs.wlu.edu/{~}levy/pubs/agi{\_}2008{\_}levy{\_}gayler.pdf$\backslash$nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.6978$\backslash$nhttp://sites.google.com/site/rgayler/agi{\_}2008{\_}levy{\_}gayler.pdf?attredirects=0},
year = {2008}
}
@article{Plate:1995:HolographicReducedRepresentations,
abstract = {Associative memories are conventionally used to represent data with very simple structure: sets of pairs of vectors. This paper describes a method for representing more complex compositional structure in distributed representations. The method uses circular convolution to associate items, which are represented by vectors. Arbitrary variable bindings, short sequences of various lengths, simple frame-like structures, and reduced representations can be represented in a fixed width vector. These representations are items in their own right and can be used in constructing compositional structures. The noisy reconstructions extracted from convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Plate, Tony A.},
doi = {10.1109/72.377968},
eprint = {arXiv:1011.1669v3},
file = {:home/ksiks/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Plate - 1995 - Holographic Reduced Representations(2).pdf:pdf},
isbn = {1045-9227},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {3},
pages = {623--641},
pmid = {18263348},
title = {{Holographic Reduced Representations}},
volume = {6},
year = {1995}
}
@book{Kleyko:2016:PatternRecognitionVector,
abstract = {Pattern recognition is an area constantly enlarging its theoretical and practical horizons. Applications of pattern recognition and machine learning can be found in many areas of the present day world including health-care, robotics, manufacturing, economics, automation, transportation, etc. Despite some success in many domains pattern recognition algorithms are still far from being close to their biological vis-a-vis – human brain. New possibilities in the area of pattern recognition may be achieved by application of biologically inspired approaches. This thesis presents the usage of a bio-inspired method of representing concepts and their meaning – Vector Symbolic Architectures – in the context of pattern recognition with possible applications in intelligent transportation systems, automation systems, and language processing. Vector Symbolic Architectures is an approach for encoding and manipulating distributed representations of information. They have previously been used mainly in the area of cognitive computing for representing and reasoning upon semantically bound information. First, it is shown that Vector Symbolic Architectures are capable of pattern classification of temporal patterns. With this approach, it is possible to represent, learn and subsequently classify vehicles using measurements from vibration sensors. Next, an architecture called Holographic Graph Neuron for one-shot learning of patterns of generic sensor stimuli is proposed. The architecture is based on implementing the Hierarchical Graph Neuron approach using Vector Symbolic Architectures. Holographic Graph Neuron shows the previously reported performance characteristics of Hierarchical Graph Neuron while maintaining the simplicity of its design. The Holographic Graph Neuron architecture is applied in two domains: fault detection and longest common substrings search. In the area of fault detection the architecture showed superior performance compared to classical methods of artificial intelligence while featuring zero configuration and simple operations. The application of the architecture for longest common substrings search showed its ability to robustly solve the task given that the length of a common substring is longer than 4{\%} of the longest pattern. Furthermore, the required number of operations on binary vectors is equal to the suffix trees approach, which is the fastest traditional algorithm for this problem. In summary, the work presented in this thesis extends understanding of the performance proprieties of distributed representations and opens the way for new applications.},
author = {Kleyko, Denis},
file = {:home/ksiks/temp/FULLTEXT01.pdf:pdf},
isbn = {978-91-7583-536-5},
title = {{Pattern Recognition with Vector Symbolic Architectures}},
year = {2016}
}

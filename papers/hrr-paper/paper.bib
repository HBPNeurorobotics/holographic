Automatically generated by Mendeley Desktop 1.16.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Recchia:2015:EncodingSequentialInformation,
abstract = {Circular convolution and random permutation have each been proposed as neurally plausible binding operators capable of encoding sequential information in semantic memory. We perform several controlled comparisons of circular convolution and random permutation as means of encoding paired associates as well as encoding sequential information. Random permutations outperformed convolution with respect to the number of paired associates that can be reliably stored in a single memory trace. Performance was equal on semantic tasks when using a small corpus, but random permutations were ultimately capable of achieving superior performance due to their higher scalability to large corpora. Finally, “noisy” permutations in which units are mapped to other units arbitrarily (no one-to-one mapping) perform nearly as well as true permutations. These findings increase the neurological plausibility of random permutations and highlight their utility in vector space models of semantics.},
annote = {Random Permutations},
author = {Recchia, Gabriel and Sahlgren, Magnus and Kanerva, Pentti and Jones, Michael N.},
doi = {10.1155/2015/986574},
file = {:media/sf{\_}vm{\_}hiwi/RandPerms.pdf:pdf},
issn = {1687-5265},
journal = {Computational Intelligence and Neuroscience},
pages = {1--18},
pmid = {25954306},
title = {{Encoding Sequential Information in Semantic Space Models: Comparing Holographic Reduced Representation and Random Permutation}},
url = {http://www.hindawi.com/journals/cin/2015/986574/},
volume = {2015},
year = {2015}
}
@incollection{Steels:2012:SymbolGroundingProblem,
abstract = {In the nineteen eighties, a lot of ink was spent on the question of$\backslash$nsymbol grounding, largely triggered by Searle�s Chinese Room story.$\backslash$nSearle�s article had the advantage of stirring up discussion about$\backslash$nwhen and how symbols could be about things in the world, whether$\backslash$nintelligence involves representa- tions or not, what embodiment means$\backslash$nand under what conditions cognition is embodied, etc. But almost$\backslash$ntwenty years of philosophical discussion have shed little light on$\backslash$nthe issue, partly because the discussion has been mixed up with arguments$\backslash$nwhether artificial intelligence was possible or not. Today I believe$\backslash$nthat sufficient progress has been made in cognitive science and AI$\backslash$nthat we can move forward and study the processes involved in representations$\backslash$ninstead of worrying about the general framework with which this should$\backslash$nbe done.},
annote = {go through that "symbol grounding problem solved" paper and report that first},
author = {Steels, Luc},
booktitle = {Symbols and EmbodimentDebates on meaning and cognition},
doi = {10.1093/acprof:oso/9780199217274.003.0012},
file = {:home/ksiks/temp/steels2008symbol.pdf:pdf},
isbn = {9780191696060},
issn = {978-0199217274},
keywords = {Color guessing game,Embodiment,Language emergence,Meanings,Symbol grounding problem,Symbols},
month = {oct},
number = {2005},
pages = {223--244},
publisher = {Oxford University Press},
title = {{The symbol grounding problem has been solved, so what's next?}},
url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199217274.001.0001/acprof-9780199217274-chapter-12},
year = {2008}
}
@article{Patyk-Lonska:2011:DistributedRepresentationsBased,
abstract = {Authors revise the concept of a distributed representation of data as well as two previously developed mod- els: Holographic Reduced Representation (HRR) and Binary Spatter Codes (BSC). A Geometric Analogue (GAc — "c" stands for continuous as opposed to its discrete version) of HRR is introduced – it employs role-filler binding based on geometric products. Atomic objects are real-valued vectors in n-dimensional Euclidean space while complex data structures belong to a hierarchy of multivectors. The paper reports on a test aimed at comparison of GAc with HRR and BSC. The test is analogous to the one proposed by Tony Plate in the mid 90s. We repeat Plate's test on GAc and compare the results with the original HRR and BSC — we concentrate on comparison of recognition percentage for the three models for comparable data size, rather than on the time taken to achieve high percentage. Results show that the best models for storing and recognizing multiple similar structures are GAc and BSC with recognition percentage highly above 90. The paper ends with remarks on perspective applications of geometric algebra to quantum algorithms.},
author = {Patyk-L{\'{o}}nska, Agnieszka and Czachor, Marek and Aerts, Diederik},
file = {:home/ksiks/temp/10.1.1.298.6032.pdf:pdf},
issn = {03505596},
journal = {Informatica (Ljubljana)},
keywords = {BSC,Distributed representation of data,Geometric algebra,HRR,Scaling},
number = {4},
pages = {407--417},
title = {{Distributed representations based on geometric Algebra: The continuous model}},
volume = {35},
year = {2011}
}
@article{Kleyko:2016:HolographicGraphNeuron,
abstract = {This article proposes the use of Vector Symbolic Architectures for implementing Hierarchical Graph Neuron, an architecture for memorizing patterns of generic sensor stimuli. The adoption of a Vector Symbolic representation ensures a one-layered design for the approach, while maintaining the previously reported properties and performance characteristics of Hierarchical Graph Neuron, and also improving the noise resistance of the architecture. The proposed architecture enables a linear (with respect to the number of stored entries) time search for an arbitrary sub-pattern.},
archivePrefix = {arXiv},
arxivId = {1501.03784},
author = {Kleyko, Denis and Osipov, Evgeny and Senior, Alexander and Khan, Asad I. and Sekercioglu, Yasar Ahmet},
doi = {10.1109/TNNLS.2016.2535338},
eprint = {1501.03784},
file = {:home/ksiks/temp/1501.03784v1.pdf:pdf},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
pages = {1--9},
title = {{Holographic Graph Neuron: A Bioinspired Architecture for Pattern Processing}},
year = {2016}
}
@inbook{Kanerva:1994:SpatterCodeEncoding,
abstract = {The Spatter Code is a high-dimensional (e.g., N = 10,000), random$\backslash$ncode that encodes “high-level concepts” in terms of their “low-level$\backslash$nattributes” so that concepts at different levels can be mixed freely. The binary$\backslash$nspatter code is the simplest. It has two N-bit codewords for each concept or$\backslash$nitem, a “high-level,” or dense, word with many randomly placed 1s and a$\backslash$n“low-level,” or sparse, word with a few (that are contained in the many). The$\backslash$ndense codewords can be used as inputs to an associative memory. The sparse$\backslash$ncodewords are used in encoding new concepts. When several items (attributes,$\backslash$nconcepts, chunks) are combined to form a new item, the two codewords for$\backslash$nthe new item are made from the sparse codewords of its constituents as follows:$\backslash$nthe new dense word is the logical {\{}OR{\}} of the constituents (i.e., their sum$\backslash$nthresholded at 0.5), and the new sparse word has 1s where the constituent$\backslash$nwords overlap (i.e., their sum thresholded at 1.5). When the parameters for the$\backslash$ncode are chosen properly, the number of 1s in the codewords is maintained as$\backslash$nnew items are encoded from combinations of old ones.},
address = {London},
author = {Kanerva, Pentti},
booktitle = {Icann'94},
doi = {10.1007/978-1-4471-2097-1_52},
editor = {Marinaro, Maria and Morasso, Pietro G},
file = {:home/ksiks/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanerva - 1994 - The spatter code for encoding concepts at many levels.pdf:pdf},
isbn = {978-3-540-19887-1},
pages = {226--229},
publisher = {Springer London},
title = {{The spatter code for encoding concepts at many levels}},
url = {http://link.springer.com/chapter/10.1007/978-1-4471-2097-1{\_}52},
year = {1994}
}
@article{Vogt:2002:PhysicalSynbolGrounding,
annote = {the actual paper},
author = {Vogt, Paul},
doi = {10.1016/S1389-0417(02)00051-7},
file = {:media/sf{\_}vm{\_}hiwi/The{\_}physical{\_}symbol{\_}grounding{\_}problem.pdf:pdf},
issn = {13890417},
journal = {Cognitive Systems Research},
month = {sep},
number = {3},
pages = {429--457},
title = {{The physical symbol grounding problem}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1389041702000517},
volume = {3},
year = {2002}
}
@article{Levy:2008:VectorSymbolicArchitectures,
abstract = {We provide an overviewofVector Symbolic Architectures (VSA), a class of structured associative memory models that offers a number of desirable features for artificial general intelligence. By directly encoding structure using familiar, computationally efficient algorithms, VSA bypasses many of the problems that have consumed unnecessary effort and attention in previous connectionist work. Example applications from opposite ends of the AI spectrum – visual map-seeking circuits and structured analogy processing – attest to the generality and power of the VSA approach in building new solutions for AI.},
author = {Levy, Simon D and Gayler, Ross W.},
file = {:home/ksiks/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy, Gayler - Unknown - Vector Symbolic Architectures A New Building Material for Artificial General Intelligence 1.pdf:pdf},
isbn = {978-1-58603-833-5},
issn = {09226389},
journal = {Proceedings of the First Conference on Artificial General Intelligence (AGI-08)},
keywords = {associative memory,binary spatter codes,connectionism,distributed representations,holographic reduced representation,vector symbolic architectures},
pages = {414--418},
pmid = {1000185213},
title = {{Vector Symbolic Architectures: A New Building Material for Artificial General Intelligence}},
url = {www.cs.wlu.edu/{~}levy/pubs/agi{\_}2008{\_}levy{\_}gayler.pdf$\backslash$nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.6978$\backslash$nhttp://sites.google.com/site/rgayler/agi{\_}2008{\_}levy{\_}gayler.pdf?attredirects=0},
year = {2008}
}
@article{Golosio:2015:CognitiveNeuralArchitecture,
abstract = {Communicative interactions involve a kind of procedural knowledge that is used by the human brain for processing verbal and nonverbal inputs and for language production. Although considerable work has been done on modeling human language abilities, it has been difficult to bring them together to a comprehensive tabula rasa system compatible with current knowledge of how verbal information is processed in the brain. This work presents a cognitive system, entirely based on a large-scale neural architecture, which was developed to shed light on the procedural knowledge involved in language elaboration. The main component of this system is the central executive, which is a supervising system that coordinates the other components of the working memory. In our model, the central executive is a neural network that takes as input the neural activation states of the short-term memory and yields as output mental actions, which control the flow of information among the working memory components through neural gating mechanisms. The proposed system is capable of learning to communicate through natural language starting from tabula rasa, without any a priori knowledge of the structure of phrases, meaning of words, role of the different classes of words, only by interacting with a human through a text-based interface, using an open-ended incremental learning process. It is able to learn nouns, verbs, adjectives, pronouns and other word classes, and to use them in expressive language. The model was validated on a corpus of 1587 input sentences, based on literature on early language assessment, at the level of about 4-years old child, and produced 521 output sentences, expressing a broad range of language processing functionalities.},
archivePrefix = {arXiv},
arxivId = {1506.03229},
author = {Golosio, Bruno and Cangelosi, Angelo and Gamotina, Olesya and Masala, Giovanni Luca},
doi = {10.1371/journal.pone.0140866},
eprint = {1506.03229},
file = {:media/sf{\_}vm{\_}hiwi/A{\_}Cognitive{\_}Neural{\_}Architecture{\_}Able{\_}to{\_}Learn{\_}and{\_}Communicate{\_}through{\_}Natural{\_}Language.pdf:pdf},
isbn = {10.1371/journal.pone.0140866},
issn = {19326203},
journal = {PLoS ONE},
number = {11},
pmid = {26560154},
title = {{A cognitive neural architecture able to learn and communicate through natural language}},
volume = {10},
year = {2015}
}
@inproceedings{DeVine:2010:Semanticoscillations,
abstract = {In computational linguistics, information retrieval and applied cognition, words and concepts are often represented as vectors in high dimensional spaces computed from a corpus of text. These high dimensional spaces are often referred to as Semantic Spaces. We describe a novel and efficient approach to computing these semantic spaces via the use of complex valued vector representations. We report on the practical implementation of the proposed method and some associated experiments. We also briefly discuss how the proposed system relates to previous theoretical work in Information Retrieval and Quantum Mechanics and how the notions of probability, logic and geometry are integrated within a single Hilbert space representation. In this sense the proposed system has more general application and gives rise to a variety of opportunities for future research.},
address = {Arlington, Virginia},
author = {{De Vine}, Lance and Bruza, Peter},
booktitle = {Quantum Informatics 2010 AAAI Fall 2010 Symposium on Quantum Informatics for Cognitive Social and Semantic Processes},
file = {:home/ksiks/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Vine, Bruza - 2010 - Semantic oscillations encoding context and structure in complex valued holographic vectors.pdf:pdf},
isbn = {9781577354901},
keywords = {AAAI Technical Report FS-10-08},
number = {48-55},
pages = {48--55},
publisher = {AAAI Press},
title = {{Semantic Oscillations: Encoding Context and Structure in Complex Valued Holographic Vectors}},
url = {http://www.aaai.org/ocs/index.php/FSS/FSS10/paper/download/2277/2680},
year = {2010}
}
@article{Harnad:1990:SymbolGroundingProblem,
abstract = {There has been much discussion recently about the scope and limits of purely symbolic models of the mind and about the proper role of connectionism in cognitive modeling. This paper describes the "symbol grounding problem": How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) iconic representations, which are analogs of the proximal sensory projections of distal objects and events, and (2) categorical representations, which are learned and innate feature detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) symbolic representations, grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g. "An X is a Y that is Z"). Connectionism is one natural candidate for the mechanism that learns the invariant features underlying categorical representations, thereby connecting names to the proximal projections of the distal objects they stand for. In this way connectionism can be seen as a complementary component in a hybrid nonsymbolic/symbolic model of the mind, rather than a rival to purely symbolic modeling. Such a hybrid model would not have an autonomous symbolic "module," however; the symbolic functions would emerge as an intrinsically "dedicated" symbol system as a consequence of the bottom-up grounding of categories' names in their sensory representations. Symbol manipulation would be governed not just by the arbitrary shapes of the symbol tokens, but by the nonarbitrary shapes of the icons and category invariants in which they are grounded. {\textcopyright} 1990.},
annote = {This is not the paper I meant, but nevertheless it is a good one. You can skim thought it if you have time.},
archivePrefix = {arXiv},
arxivId = {arXiv:cs.AI/9906002},
author = {Harnad, Stevan},
doi = {10.1016/0167-2789(90)90087-6},
eprint = {9906002},
file = {:home/ksiks/temp/harnad90.pdf:pdf},
isbn = {0167-2789},
issn = {01672789},
journal = {Physica D: Nonlinear Phenomena},
number = {1-3},
pages = {335--346},
primaryClass = {arXiv:cs.AI},
title = {{The symbol grounding problem}},
volume = {42},
year = {1990}
}
@article{Levy:2013:LearningBehaviorHierarchies,
abstract = {We propose a knowledge-representation architecture allowing a robot to learn arbitrarily complex, hierarchical / symbolic relationships between sensors and actuators. These relationships are encoded in high-dimensional, low-precision vectors that are very robust to noise. Low-dimensional (single-bit) sensor values are projected onto the high-dimensional representation space using low-precision random weights, and the appropriate actions are then computed using elementwise vector multiplication in this space. The high-dimensional action representations are then projected back down to low-dimensional actuator signals via a simple vector operation like dot product. As a proof-of-concept for our architecture, we use it to implement a behavior-based controller for a simulated robot with three sensors (touch sensor, left/right light sensor) and two actuators (wheels). We conclude by discussing the prospects for deriving such representations automatically. Copyright {\textcopyright} 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.},
author = {Levy, Simon D. and Bajracharya, Suraj and Gayler, Ross W.},
file = {:media/sf{\_}vm{\_}hiwi/Learning{\_}Behavior{\_}Hierarchies{\_}via{\_}High-Dimensional{\_}Sensor{\_}Projection.pdf:pdf},
isbn = {9781577356233},
journal = {AAAI Workshop - Technical Report},
keywords = {Behavior-based robotics,Knowledge representation,Vector symbolic architectures},
pages = {25--27},
title = {{Learning behavior hierarchies via high-dimensional sensor projection}},
volume = {WS-13-12},
year = {2013}
}
@article{Plate:1995:HolographicReducedRepresentations,
abstract = {Associative memories are conventionally used to represent data with very simple structure: sets of pairs of vectors. This paper describes a method for representing more complex compositional structure in distributed representations. The method uses circular convolution to associate items, which are represented by vectors. Arbitrary variable bindings, short sequences of various lengths, simple frame-like structures, and reduced representations can be represented in a fixed width vector. These representations are items in their own right and can be used in constructing compositional structures. The noisy reconstructions extracted from convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Plate, Tony A.},
doi = {10.1109/72.377968},
eprint = {arXiv:1011.1669v3},
file = {:home/ksiks/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Plate - 1995 - Holographic Reduced Representations(2).pdf:pdf},
isbn = {1045-9227},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {3},
pages = {623--641},
pmid = {18263348},
title = {{Holographic Reduced Representations}},
volume = {6},
year = {1995}
}
@book{Kleyko:2016:PatternRecognitionVector,
abstract = {Pattern recognition is an area constantly enlarging its theoretical and practical horizons. Applications of pattern recognition and machine learning can be found in many areas of the present day world including health-care, robotics, manufacturing, economics, automation, transportation, etc. Despite some success in many domains pattern recognition algorithms are still far from being close to their biological vis-a-vis – human brain. New possibilities in the area of pattern recognition may be achieved by application of biologically inspired approaches. This thesis presents the usage of a bio-inspired method of representing concepts and their meaning – Vector Symbolic Architectures – in the context of pattern recognition with possible applications in intelligent transportation systems, automation systems, and language processing. Vector Symbolic Architectures is an approach for encoding and manipulating distributed representations of information. They have previously been used mainly in the area of cognitive computing for representing and reasoning upon semantically bound information. First, it is shown that Vector Symbolic Architectures are capable of pattern classification of temporal patterns. With this approach, it is possible to represent, learn and subsequently classify vehicles using measurements from vibration sensors. Next, an architecture called Holographic Graph Neuron for one-shot learning of patterns of generic sensor stimuli is proposed. The architecture is based on implementing the Hierarchical Graph Neuron approach using Vector Symbolic Architectures. Holographic Graph Neuron shows the previously reported performance characteristics of Hierarchical Graph Neuron while maintaining the simplicity of its design. The Holographic Graph Neuron architecture is applied in two domains: fault detection and longest common substrings search. In the area of fault detection the architecture showed superior performance compared to classical methods of artificial intelligence while featuring zero configuration and simple operations. The application of the architecture for longest common substrings search showed its ability to robustly solve the task given that the length of a common substring is longer than 4{\%} of the longest pattern. Furthermore, the required number of operations on binary vectors is equal to the suffix trees approach, which is the fastest traditional algorithm for this problem. In summary, the work presented in this thesis extends understanding of the performance proprieties of distributed representations and opens the way for new applications.},
author = {Kleyko, Denis},
file = {:home/ksiks/temp/FULLTEXT01.pdf:pdf},
isbn = {978-91-7583-536-5},
title = {{Pattern Recognition with Vector Symbolic Architectures}},
year = {2016}
}
@article{Eliasmith:2012:LargeScaleModel,
abstract = {A central challenge for cognitive and systems neuroscience is to relate the incredibly complex behavior of animals to the equally complex activity of their brains. Recently described, large-scale neural models have not bridged this gap between neural activity and biological function. In this work, we present a 2.5-million-neuron model of the brain (called "Spaun") that bridges this gap by exhibiting many different behaviors. The model is presented only with visual image sequences, and it draws all of its responses with a physically modeled arm. Although simplified, the model captures many aspects of neuroanatomy, neurophysiology, and psychological behavior, which we demonstrate via eight diverse tasks.},
author = {Eliasmith, Chris and Stewart, Terrence C and Choo, Xuan and Bekolay, Trevor and DeWolf, Travis and Tang, Charlie and Rasmussen, Daniel and Tang, Yichuan and Tang, Charlie and Rasmussen, Daniel},
doi = {10.1126/science.1225266},
file = {:home/ksiks/temp/eliasmith.2012.pdf:pdf},
isbn = {0036-8075},
issn = {0036-8075},
journal = {Science},
keywords = {Behavior,Brain,Brain: anatomy {\&} histology,Brain: physiology,Humans,Models,Neural Networks (Computer),Neurological,Software},
number = {6111},
pages = {1202--1205},
pmid = {1000106157},
title = {{A Large-Scale Model of the Functioning Brain}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1225266$\backslash$nhttp://www.ncbi.nlm.nih.gov/pubmed/23197532},
volume = {338},
year = {2012}
}
@article{Paradis:2013:FindingSemanticEquivalence,
abstract = {The challenges of machine semantic understanding have not yet been satisfactorily solved by automated methods. In our approach, the semantics and syntax of words, phrases and documents are represented by deep semantic vectors that capture both the structure and semantic meaning of the language. Our experiment reproduces the experiment done by Patwardhan and Pedersen 2006, but uses random index vectors for the words, glosses and tweets. Our model first determines random index vectors from glosses and definitions for words from WordNet. From these foundational semantic vectors, random index vectors that represent phrases, sentences or tweets are determined. Our set of algorithms relies on high-dimensional distributed representations, and their effectiveness and versatility derive from the unintuitive properties of such representations: from the mathematical properties of high-dimensional spaces. High-dimensional vector representations have been used successfully in modeling human cognition, such as memory and learning. Our semantic vectors are high-dimensional and capture the meaning of a language expression, such as a word, phrase, query, news article, story or a message. A key benefit of our method is that the dimensionality of the vectors remains constant as we add data; this also allows good generalization to rarely seen words, which "borrow strength" from their more frequent neighbors. {\textcopyright} 2013 The Authors. Published by Elsevier B.V.},
annote = {check},
author = {Paradis, Rosemary D. and Guo, Jinhong K. and Moulton, Jack and Cameron, David and Kanerva, Pentti},
doi = {10.1016/j.procs.2013.09.302},
file = {:home/ksiks/temp/1-s2.0-S1877050913011022-main.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Artificial intelligence,Computational linguistics,High-dimension,Machine learning,Random index,Social networking},
pages = {454--459},
publisher = {Elsevier Masson SAS},
title = {{Finding semantic equivalence of text using random index vectors}},
url = {http://dx.doi.org/10.1016/j.procs.2013.09.302},
volume = {20},
year = {2013}
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HRR - Reinforcement learning\n",
    "===========\n",
    "\n",
    "Distal reward problem solution using HRRs\n",
    "----------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from core import HRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine an experiment in which mouse starts in the middle of corridor by observing light that can be red or green. After that he chooses to go left or right to the end of the corridor, where there might or not be cheese. Initial light gives information about the direction in which cheese is: red light means that cheese is left and green light means it is right.\n",
    "\n",
    "Lets create our symbolic representations for sensory input, actions and reward signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HRR.reset_kernel()\n",
    "HRR.default_size = 1000\n",
    "HRR.verbose = False\n",
    "red_light, green_light = HRR(\"red_light\"), HRR(\"green_light\")\n",
    "left_turn, right_turn = HRR(\"left_turn\"), HRR(\"right_turn\")\n",
    "reward, punishment = HRR(\"reward\"), HRR(\"punishment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After first run mouse sees red light, choses to go left and receives a reward, which is memorized like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run1 = red_light * left_turn * reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just this experience, we can try to check what is the current policy that mouse has learned just from a single trial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeing red_light mouse would choose action: left_turn\n",
      "Seeing green_light mouse would choose action: red_light\n"
     ]
    }
   ],
   "source": [
    "print(\"Seeing red_light mouse would choose action: {}\".format((run1 % reward) / red_light))\n",
    "print(\"Seeing green_light mouse would choose action: {}\".format((run1 % reward) / green_light))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, second answer doesn't make sense since mouse hasn't explored that case yet.\n",
    "\n",
    "Now we will suppose that mouse had 4 chances to perform experiment, covering (idealy) the whole state-action space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run2 = red_light * right_turn * punishment\n",
    "run3 = green_light * left_turn * punishment\n",
    "run4 = green_light * right_turn * reward\n",
    "\n",
    "all_runs = run1 + run2 + run3 + run4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will repeat query from previous step, asking the complete policy all_runs what are the optimal actions for both sensory inputs for reaching reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeing red_light mouse would choose action: left_turn\n",
      "Seeing green_light mouse would choose action: right_turn\n"
     ]
    }
   ],
   "source": [
    "print(\"Seeing red_light mouse would choose action: {}\".format(all_runs / ( reward * red_light)))\n",
    "print(\"Seeing green_light mouse would choose action: {}\".format(all_runs / (reward * green_light)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, mouse knows how to navigate the maze optimally and the sensor->action policy is stored in all_runs HRR.\n",
    "\n",
    "Lets assume that there were two signal lights at the beginning, one of them having same colors and function as before experiment modification and another one being a completely irrelevant distractor. Second light can be yellow_light or blue_light, whoch is selected randomly at each run and cheese position doesn't depend on it.\n",
    "\n",
    "In this case, mouse will memorize both percieved stimuli (red/green + yellow/blue), so the memory after couple of runs might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yellow_light, blue_light = HRR(\"yellow_light\"), HRR(\"blue_light\")\n",
    "\n",
    "run1 = (red_light + yellow_light) * left_turn * reward \n",
    "run2 = (red_light + blue_light) * left_turn * reward\n",
    "run3 = (red_light + yellow_light) * right_turn * punishment\n",
    "run4 = (green_light + yellow_light) * right_turn * reward\n",
    "#run5 = green_light * blue_light * right_turn * reward\n",
    "all_runs = run1 + run2 + run3 + run4# + run5\n",
    "\n",
    "HRR.verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from green_light is 0.0549922588493\n",
      "Distance from right_turn is 0.131932534883\n",
      "Distance from blue_light is -0.0802718462334\n",
      "Distance from red_light is 0.0955334244916\n",
      "Distance from yellow_light is 0.00319471517103\n",
      "Distance from left_turn is 0.11997377468\n",
      "Distance from reward is 0.0637256176354\n",
      "Distance from punishment is -0.00664726946128\n",
      "Common sensory stimuli was: {'left_turn': 0.11997377467958976, 'right_turn': 0.13193253488347595}\n"
     ]
    }
   ],
   "source": [
    "print(\"Common sensory stimuli was: {}\".format(all_runs / (reward * (green_light + blue_light))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HRR - Action selection\n",
    "====\n",
    "\n",
    "Lets define a set of symbolic sensory input values for **tactile** sense (touch_left, no_touch), **eyes** (cheese_left, cheese_right, no cheese) and set of **motor actions** for a wheel rotation (forward, backward). \"Program\" for approaching the cheese and moving away from wall would look as follows:\n",
    "\n",
    "        if (eyes == cheese_left):\n",
    "           left_wheel = forward\n",
    "           right_wheel = backward\n",
    "\n",
    "       if (eyes == cheese_right):\n",
    "           left_wheel = backward\n",
    "           right_wheel = forward\n",
    "\n",
    "       if (tactile == touch_left):\n",
    "           left_wheel = backward\n",
    "           right_wheel = forward\n",
    "    \n",
    "       if (tactile == touch_right):\n",
    "           left_wheel = forward\n",
    "           right_wheel = backward\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from core import HRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will declare all of our symbols first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HRR.reset_kernel()\n",
    "HRR.default_size = 100\n",
    "HRR.verbose = False\n",
    "\n",
    "forward = HRR(\"forward\")\n",
    "backward = HRR(\"backward\")\n",
    "no_motion = HRR(\"no_motion\")\n",
    "touch_left, touch_right, no_touch = HRR(\"touch_left\"), HRR(\"touch_right\"), HRR(\"no_touch\")\n",
    "cheese_left, cheese_right, no_cheese = HRR(\"cheese_left\"), HRR(\"cheese_right\"), HRR(\"no_cheese\")\n",
    "\n",
    "left_wheel = HRR(\"left_wheel\")\n",
    "right_wheel = HRR(\"right_wheel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a program for handling left_touch and right_touch events on haptic sensors separatelly. \n",
    "\n",
    "\\begin{equation}\n",
    "\\center\n",
    "left_touch_program=T_L \\cross (A_F \\cross W_L + A_B \\cross W_R)\n",
    "\\end{equation\n",
    "\n",
    "For left_touch event we want to set left_wheel to forward and right_wheel to backward, so we move away from obstacle. We will do this using **binding operation**. We want to do opposite thing for right_touch, to result in motion that drives agent away from obstacle. \n",
    "\n",
    "Combining these two cases into a single program is done with **superposition**:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44537222  0.35882483 -0.23232629  0.09366966 -0.28087172 -0.48398365\n",
      " -0.4333629   0.27101847  0.28315966  0.32739683  0.11767    -0.05710428\n",
      "  0.02700785 -0.399639   -0.23841391  0.06135454  0.31311104  0.14712888\n",
      "  0.24306917  0.27911278 -0.42853367 -0.3276419   0.03617653  0.16088609\n",
      "  0.21274095  0.20611933  0.32175012 -0.07516708 -0.06277486 -0.4443288\n",
      " -0.15842378 -0.18774508  0.34443934  0.26499677  0.32492067  0.13058903\n",
      " -0.42433628 -0.31470214 -0.41283554 -0.27447538  0.05107525  0.45240108\n",
      "  0.10323443  0.08975298  0.02096106 -0.0761948  -0.47203608 -0.44010774\n",
      "  0.07624263 -0.10761523  0.25971956  0.03903721 -0.27842012 -0.24407275\n",
      " -0.52718159  0.03637453  0.03172395  0.48716245  0.06852792  0.31758278\n",
      " -0.20993405 -0.31811356 -0.38908562 -0.55858502 -0.11570875  0.19025514\n",
      "  0.13819692  0.47863232 -0.10351411  0.27072206 -0.5578898  -0.22527813\n",
      " -0.12253516  0.12879428  0.14031171  0.13819048  0.34170957 -0.03874256\n",
      " -0.22116381 -0.40607057  0.10114132 -0.43566521  0.22365184  0.22966486\n",
      "  0.14802148  0.19608723 -0.22185842 -0.09078687 -0.13355596 -0.0571588\n",
      "  0.12778442  0.35818696  0.08517203  0.02377559  0.06051194 -0.64234203\n",
      " -0.12105382 -0.33849896  0.06732194  0.22253447]\n"
     ]
    }
   ],
   "source": [
    "left_touch_program = touch_left * (left_wheel * forward + right_wheel * backward)\n",
    "\n",
    "right_touch_program = touch_right * (left_wheel * backward+ right_wheel * forward)\n",
    "\n",
    "avoidance_program = left_touch_program + right_touch_program\n",
    "\n",
    "print(avoidance_program.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test avoidance program. Let's assume we have detected symbolic input \"touch_right\" from our tactile sensor. In order to read all of the actions from avoidance program for this sensory input we will probe it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left wheel: backward\n",
      "Right wheel: forward\n"
     ]
    }
   ],
   "source": [
    "left_wheel_action = avoidance_program / (touch_right * left_wheel)\n",
    "print('Left wheel: {}'.format(left_wheel_action))\n",
    "\n",
    "right_wheel_action = avoidance_program / (touch_right * right_wheel)\n",
    "print('Right wheel: {}'.format(right_wheel_action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform the query as 2-step process, there we can obtain joint representation for all actions that need to be performed when touch_right has been sensed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09627867  0.23448294  0.50859683  0.60035041  0.33385294  0.10352229\n",
      " -0.439971   -0.57979944 -0.3372838  -0.03248078  0.26646285  0.39193102\n",
      "  0.15599485  0.00673835  0.00685249 -0.43764906 -0.34562794 -0.1651218\n",
      "  0.61955286  0.25049943  0.4647431  -0.32108515 -0.03686391 -0.43372397\n",
      " -0.66189257 -0.11845402  0.43034372  0.38254897  0.15680907  0.37838338\n",
      "  0.0410214  -0.73979956 -0.45604494 -0.10594229  0.18118695  0.14780548\n",
      "  0.50614592  0.34397667 -0.3482567  -0.34230276 -0.67846352 -0.02635783\n",
      " -0.05126454  0.12835354  0.28635477  0.48716178 -0.01463486 -0.36855184\n",
      " -0.50570439 -0.42900249  0.0996987   0.11855436  0.21533274  0.25632106\n",
      " -0.18005986 -0.22606901 -0.46614787 -0.16953232 -0.01407183 -0.1034648\n",
      "  0.33864231  0.57970582  0.08234457 -0.48757132 -0.36689228 -0.59486836\n",
      " -0.44901753 -0.17832053  0.27589703  0.30626424  0.18001431 -0.01878266\n",
      " -0.194415   -0.30740008 -0.44504998 -0.373239    0.3217952   0.29504135\n",
      " -0.04777484  0.10611516 -0.25404163 -0.38611481 -0.4162789  -0.27243665\n",
      " -0.01651934  0.44965148  0.38319576  0.34731596  0.03458441 -0.25165939\n",
      " -0.43003279 -0.02132838 -0.16271132  0.15175717  0.33638567  0.50255474\n",
      " -0.00992884 -0.2936723  -0.25249738 -0.50984473]\n"
     ]
    }
   ],
   "source": [
    "all_touch_right_actions = avoidance_program % touch_right\n",
    "print(all_touch_right_actions.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine the individual actions for wheels, we will probe the result of previous operation with resprective symbolic representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action for left wheel: backward\n",
      "Action for right wheel: forward\n"
     ]
    }
   ],
   "source": [
    "left_wheel_action = all_touch_right_actions / left_wheel\n",
    "print('Action for left wheel: {}'.format(left_wheel_action))\n",
    "right_wheel_action = all_touch_right_actions / right_wheel\n",
    "print('Action for right wheel: {}'.format(right_wheel_action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
